{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
    "!unzip YouTube-Spam-Collection-v1.zip\n",
    "!mkdir -p data/youtube\n",
    "!mv *.csv data/youtube\n",
    "!rm -rf __MACOSX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Youtube01-Psy.csv\t Youtube03-LMFAO.csv   Youtube05-Shakira.csv\n",
      "Youtube02-KatyPerry.csv  Youtube04-Eminem.csv\n"
     ]
    }
   ],
   "source": [
    "!ls data/youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_comments = dd.read_csv(\"data/youtube/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1956, 5),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.compute(youtube_comments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Classification Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam    1005\n",
       "ham      951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_comments['CLASS']\\\n",
    "    .mask(youtube_comments['CLASS'] == 1, 'spam')\\\n",
    "    .mask(youtube_comments['CLASS'] == 0, 'ham')\\\n",
    "    .compute()\\\n",
    "    .value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Many Non-Spam Comments Contain The Word 'check'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS</th>\n",
       "      <th>contains_check</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ham</th>\n",
       "      <th>False</th>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">spam</th>\n",
       "      <th>False</th>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      comments\n",
       "CLASS contains_check          \n",
       "ham   False                932\n",
       "      True                  19\n",
       "spam  False                544\n",
       "      True                 461"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_comments[['CLASS', 'CONTENT']]\\\n",
    "    .assign(\n",
    "        contains_check=youtube_comments['CONTENT'].str.lower().str.contains('check'),\n",
    "        CLASS=youtube_comments['CLASS']\\\n",
    "                .mask(youtube_comments['CLASS'] == 1, 'spam')\\\n",
    "                .mask(youtube_comments['CLASS'] == 0, 'ham')\n",
    "    )\\\n",
    "    .groupby(['CLASS', 'contains_check'])\\\n",
    "    .count()\\\n",
    "    .compute()\\\n",
    "    .rename(columns={\n",
    "        'CONTENT': 'comments'\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platforms, Languages, and Libraries. What have we learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the benfit extended by the tools we've been using this past week have been determined by the development goals and ecological context within which each project was built. While, for example, dask and spark both broadly solve the same issues of distributed computation on large data. Spark, by way of being a Java project and receiving heavy adoption in industry, is large and vertically integrated, whereas dask aims to solve only the issue of coordinating a distributed version of the pandas framework.\n",
    "\n",
    "With respect to these technologies AWS EMR and DataBricks have popped up as simplifying platforms upon which one can outsource the management of compute clusters, and simply worry about the computations themselves.\n",
    "\n",
    "Of these technologies, I prefer those with a small footprint; Python, SQL, or Dask. The fact that these have remained well-segmented is what allows them to be composed so well, and therein allows for more powerful abstraction. Specifically, these technologies are useful when the time exists for building out a problem specific solution, otherwise large and wide-ranging technologies like Spark allow for immediate integration and adoption, making them useful for those who just want to solve the problem at hand and nothing more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
