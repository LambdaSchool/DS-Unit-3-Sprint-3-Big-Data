{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization with Dask\n",
    "I'm going to revisit the FMA music database and see if I can run regressions faster with parallel computation on AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incantations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 226 ms, total: 2.5 s\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The first line contains the names for all the columns except for the very first one, which is somehow alone\n",
    "# in the second row.\n",
    "\n",
    "# Take columns names from first row\n",
    "tracks = pd.read_csv('tracks.csv', header=1)\n",
    "\n",
    "# Manually name the first column, and remove the first line where that name used to be.\n",
    "tracks.rename(columns={'Unnamed: 0':'track_id'}, inplace=True)\n",
    "tracks.drop(index=0, inplace=True)\n",
    "tracks.head()\n",
    "\n",
    "# Put the genre_top column at the beginning\n",
    "tracks = tracks.reindex(columns=(['genre_top'] + list([a for a in tracks.columns if a != 'genre_top'])))\n",
    "\n",
    "# Remove any columns that don't have an entry for genre_top\n",
    "tracks = tracks.dropna(subset=['genre_top'])\n",
    "\n",
    "garbage_columns = ['track_id','id', 'information','comments.1','title','bio',\n",
    "                   'members','website','wikipedia_page','split','subset',\n",
    "                   'comments.2','genres','genres_all','information.1','license','title.1']\n",
    "\n",
    "tracks2 = tracks.drop(columns=garbage_columns)\n",
    "tracks_numeric = tracks2.select_dtypes('number')\n",
    "tracks_numeric = tracks_numeric.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I tried repeating all these cleanup steps with a Dask DataFrame instead of Pandas, but it turns out that some of those operations are not in the Dask subset of pandas operations.  Lesson learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 21 Âµs, total: 12.3 ms\n",
      "Wall time: 11.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = tracks2['genre_top']\n",
    "X = tracks_numeric\n",
    "\n",
    "# Split into train and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "import dask_searchcv as dcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {'C':[1, 10, 100]}\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr',\n",
    "                            solver='liblinear',\n",
    "                            max_iter=500)\n",
    "\n",
    "search_dask = dcv.GridSearchCV(model, param_space, cv=3)\n",
    "search_sklearn = GridSearchCV(model, param_space, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 0 ns, total: 1min 28s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1, param_grid={'C': [1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Executing a grid search with the original sklearn function\n",
    "search_sklearn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 75.9 ms, total: 2min 6s\n",
      "Wall time: 30.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cache_cv=True, cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       iid=True, n_jobs=-1, param_grid={'C': [1, 10, 100]}, refit=True,\n",
       "       return_train_score='warn', scheduler=None, scoring=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Executing a grid search with the new Dask function\n",
    "search_dask.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like parallelizing our computation brought the time down from 88 seconds to 30s.  Not bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
