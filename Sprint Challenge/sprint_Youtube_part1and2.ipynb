{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAssignment Part 1: \\n\\nFor this Sprint Challenge, you don\\'t need to create a Dask Distributed Client. You can just use a Dask Dataframe.\\n\\n- Load the five csv files into one Dask Dataframe. It should have a length of 1956 rows, and 5 columns.\\n\\n- Use the Dask Dataframe to compute the counts of spam (1005 comments) versus the counts of legitimate comments (951).\\n\\n- Spammers often tell people to check out their stuff! When the comments are converted to lowercase, then 461 spam comments contain the word \"check\", versus only 19 legitimate comments which contain the word \"check.\" Use the Dask Dataframe to compute these counts.\\n\\n- Optional bonus\\nTo score a 3, do extra work, such as creating visualizations with this dataset.\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Assignment Part 1: \n",
    "\n",
    "For this Sprint Challenge, you don't need to create a Dask Distributed Client. You can just use a Dask Dataframe.\n",
    "\n",
    "- Load the five csv files into one Dask Dataframe. It should have a length of 1956 rows, and 5 columns.\n",
    "\n",
    "- Use the Dask Dataframe to compute the counts of spam (1005 comments) versus the counts of legitimate comments (951).\n",
    "\n",
    "- Spammers often tell people to check out their stuff! When the comments are converted to lowercase, then 461 spam comments contain the word \"check\", versus only 19 legitimate comments which contain the word \"check.\" Use the Dask Dataframe to compute these counts.\n",
    "\n",
    "- Optional bonus\n",
    "To score a 3, do extra work, such as creating visualizations with this dataset.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using * to select all files and load them\n",
    "dask_df = dd.read_csv(\"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking rows\n",
    "len(dask_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Delayed('int-dec119f7-de90-4e6d-8fd0-d6aea82441cb'), 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming columns of 5\n",
    "dask_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if files loaded properly\n",
    "dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Use the Dask Dataframe to compute the counts of spam (1005 comments) \n",
    "# versus the counts of legitimate comments (951)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    1005\n",
       "AUTHOR        1005\n",
       "DATE           760\n",
       "CONTENT       1005\n",
       "CLASS         1005\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_df[dask_df['CLASS'] == 1].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    951\n",
       "AUTHOR        951\n",
       "DATE          951\n",
       "CONTENT       951\n",
       "CLASS         951\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_df[dask_df['CLASS'] == 0].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Spammers often tell people to check out their stuff! When the comments are converted to lowercase, \n",
    "# then 461 spam comments contain the word \"check\", versus only 19 legitimate comments which contain the word \"check.\" \n",
    "# Use the Dask Dataframe to compute these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting all lowercase \n",
    "dask_df['CONTENT'] = dask_df['CONTENT'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    461\n",
       "AUTHOR        461\n",
       "DATE          294\n",
       "CONTENT       461\n",
       "CLASS         461\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using contain method to find # of check & class 1\n",
    "dask_df[(dask_df['CONTENT'].str.contains('check')) & (dask_df['CLASS'] == 1)].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    19\n",
       "AUTHOR        19\n",
       "DATE          19\n",
       "CONTENT       19\n",
       "CLASS         19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using contain method to find # of check & class 0\n",
    "dask_df[(dask_df['CONTENT'].str.contains('check')) & (dask_df['CLASS'] == 0)].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Bonus: Make Visualizations with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Big Data Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you explore deeper into the rabbit hole of data science you'll begin to notice that your basic computer can't process the plethora of gigabytes, terabytes and even in some instances petahashes of data efficiently. An opportunity for capitalists to solve through offering distributed computing cloud services such as Amazon AWS with it's AWS SageMaker and AWS EMR. Other players such as Databricks are also throwing their hats into this ring as well as Microsoft with it's Azure. \n",
    "    Amazon AWS SageMaker is a machine learning platform with kernels for Python and Anaconda with some Apache Spark. It allows a developer or data scientist with the computing resources to build, train and deploy machine learning models very quickly by allowing you to take advantage of 64 Gigs of RAM or with as much as you can afford per hour.\n",
    "    Amazon AWS EMR is a managed cluster platform that allows the running of \"BIG DATA\" frameworks such as Apache Hadoop and Spark. EMR allows Hadoop and Spark to process, run and analyze extremely large amounts of data. You can run Hadoop or Spark on EC2 instances with a super user friendly interface. Another advantage of EMR is that you can move your data into other Amazon AWS platforms and databases for further manipulation or storage. \n",
    "    Databricks, made by the creators of Spark, have created a free, user friendly environment to work with similarily what SageMaker and EMR do but through notebooks that are easily interactable and designed for making visualizations. It's an easier and cost friendly way of approaching or getting started in cloud distributions and Spark without paying a kidney, a promised 1st round pick and maybe a third mortgage on your house the way AWS charges. \n",
    "    The cloud platforms as to which we play around in are cool and convenient however I enjoy the technologies that allow us to manipulate the big data. My favorites, as I've gone from not having any idea and interaction with machine learning and aritifical intelligence to diving as deep as the Mariana Trench briefly to get a glimpse of it, has to be Dask and Numba. \n",
    "    Don't kill me yet for saying Dask, as it may not be fully matured yet as a flexible library thats used for parallel computing in Python but with Numba it can be a wonderful tool, which allows you to 'scale up' and 'scale out' as well. Numba, which is an open-source JIT compiler that translates Python and NumPy  into fast machine code. Numba also allows takes Python math, translates it, so that its used to optimize machine code. This allows libraries like SciPy and NumPy to operate at a faster rate because Numba offers a range of options for parallelizing Python code for CPUs and GPUs with rarely minor code changes. \n",
    "    I don't have crazy amounts of time on this sprint, but I like the Scala language as well, almost as much as Python but I'll break it down in a blog post later. I just wanted to jot that down here.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
