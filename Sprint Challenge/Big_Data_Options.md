# Big Data Options

An important part of a data scientist's job is selecting appropraite tools to use in a given situation. In paricular, selecting an appropriate set of tools to work with big data is critical. There are a litinary of tools available to work with big data; however, each tool has strengths and weaknesses. At a high-level, many of these tools either help data scientists 'scale up' or 'scale out'. Scale up essentially means run a workload on a more powerful computer, AWS SageMaker is an example of this type of service. Scale out means distirbuting a given workload over a number of computers, Spark is an example of this service. 

In the below table, I summarize my general perspectiveo on which big data tools are best suited for common situations faced by data scientists.

| Size of Data | Likely Best Approach | Strengths                                                                                                                                                                            | Additional Tools                                                                                                                                                                                                                                                   |
|--------------|----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <4GB         | Run Locally          | Essentially free, no internet connection required.                                                                                                                                   | Standard Python DS Stack (pandas, numpy, sklearn, jupyter)                                                                                                                                                                                                         |
| <1TB         | AWS SageMaker        | SageMaker makes it simple to run jupyter notebooks on virtual machines through AWS. For relatively low $/hour SageMaker provides access to extremely large amounts of compute power. | Numba - A python library that accelerates computation (particularly Numpy code) by compiling closer to machine code level. Dask - A python library which enables extremely efficient parallel computations on single machines by leveraging their multi-core CPUs. |
| >=1TB        | Spark via DataBricks | DataBricks is a developer-friendly platforms to work with Spark. Spark manages and coordinates the execution of tasks on data across a cluster of computers.                         | Scala - Spark has APIs which support a number of languages, but Scala is the best-supported and receives features first.                                                                                                                                           |