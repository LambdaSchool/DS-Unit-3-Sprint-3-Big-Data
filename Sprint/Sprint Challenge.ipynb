{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    1956\n",
       "AUTHOR        1956\n",
       "DATE          1711\n",
       "CONTENT       1956\n",
       "CLASS         1956\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(\"*.csv\")\n",
    "df.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.CLASS.value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    544\n",
       "True     461\n",
       "Name: CONTENT, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.CONTENT.str.lower().str.contains(\"check\").value_counts().compute()\n",
    "spam = df[df['CLASS']==1].compute()\n",
    "spam['CONTENT'].str.lower().str.contains('check').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well so far AWS is the most robust feature rich platform for large data crunching, but databricks is easier to use and set up. LIke many things in tech there are tradeoffs that must be made when using different technologies. If I were to pick between AWS sagemaker/EMR and databricks and I could never use the other, i'd go with AWS because it has so much overhead. I'll never have to worry about compute power and I get many other features. Dask is great because it's basically pandas with spark capabilities, I love how similar it is to pandas. Spark on the other hand, is used in many buisnesses to do large data crunching and if i'm trying to get a job being able to use spark is a big plus on my resume. Scala has been fun it's like a mix of java and python, it definitly has it's place but python is the best and i'm sticking with it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
