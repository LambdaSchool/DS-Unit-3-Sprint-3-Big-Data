# DS-Unit-3-Sprint-3-Big-Data
LOOK HERE FOR SPRINT CHALLENGE PART 2

I don't feel like I've explored the options for scaling deeply enough, but from what I've done I prefer AWS Sagemaker as a platform to Databricks, Dask as a library to Spark, and python and pure SQL as languages to Scala. These mostly come up to awkwardness with switching between too different formats and how many different things they need.

AWS "scales out", meaning it divides the computational load onto multiple computers at a minor cost. Free Databricks mimics this but isn't as powerful. I prefer that AWS continues to run clusters over Databricks shutting them down after inactivity because there are machine learning projects that could actually need the time (like a model evaluation with lots of possible configurations). I prefer AWSeven for smaller projects because it's much easier to keep things organized when clusters aren't disabled for what appears to be for good.

Neverminding that Spark is developed for Scala first and best handled in the language, I feel like I am seceding some control over the dataframe and visualizations when using it. The best way to make selections in Spark is to convert the dataframe into a tempview compatible with SQL. I feel like I could come to prefer that in time because SQL is so direct and it removes the problem of needing to restart runtime because you made a permanent change in the dataframe that you regret. At the moment it feels like more work for less. Visualizations can be generated for you with display(), but they're ugly and I'm the sort of person who needs to customize every aspect. Dask is also a scale up dataframe and feels closer to what I'm used to.

Scala is a lot more explicit that when you make a variable or change you are making an OBJECT with THIS OBJECT TYPE that has these qualities. I may have gotten the grasp of how object types interact quicker had I started here. But I also would have found the increased reliance on immutable values obnoxious.
