{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask import compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('Youtube*.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute(df.shape)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 1005 \n",
      " Legitimate 951\n"
     ]
    }
   ],
   "source": [
    "print(\"Spam:\", len(df[df.CLASS == 1]), \"\\n Legitimate\", len(df[df.CLASS == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CONTENT_lowercase'] = df['CONTENT'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 spam comments contain the word 'check'\n",
      "19 legitimate comments contain the word 'check'\n"
     ]
    }
   ],
   "source": [
    "print(df[df.CLASS == 1].CONTENT_lowercase.str.contains('check').sum().compute(), \"spam comments contain the word 'check'\")\n",
    "print(df[df.CLASS == 0].CONTENT_lowercase.str.contains('check').sum().compute(), \"legitimate comments contain the word 'check'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba can be used to \"scale up\" when dealing with data that can fit into a laptop. However, when this data is passed into functions, this can significantly slow down wait time. Numba can be used to convert python code into lower level code to speed up wait time. Dask, AWS SageMaker, and DataBricks are used to \"scale out\" when data is too big to fit into a single computer. DataBricks uses Scala to distribute script execution among several \"Executors\" to parallelize jobs also known as \"scaling out.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
