{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logisticregression",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinn-dougherty/DS-Unit-2-Sprint-3-Advanced-Regression/blob/master/module1-logistic-regression/logisticregression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nF2dsyLVpa_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# main source: read.pudn.com/downloads158/ebook/702714/Larry Wasserman_ALL OF Statistics.pdf#page=243\n",
        "# idk if this is the same edition i'm using. \n",
        "# i also used ESL (Hastie et al) and that's where the dataset came from, but that treatment is more useful for when there's more than 2 classes. \n",
        "\n",
        "dat_url = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data'\n",
        "\n",
        "'''A retrospective sample of males in a heart-disease high-risk region\n",
        "of the Western Cape, South Africa. There are roughly two controls per\n",
        "case of CHD. Many of the CHD positive men have undergone blood\n",
        "pressure reduction treatment and other programs to reduce their risk\n",
        "factors after their CHD event. In some cases the measurements were\n",
        "made after these treatments. These data are taken from a larger\n",
        "dataset, described in  Rousseauw et al, 1983, South African Medical\n",
        "Journal. \n",
        "\n",
        "sbp\t\tsystolic blood pressure\n",
        "tobacco\t\tcumulative tobacco (kg)\n",
        "ldl\t\tlow densiity lipoprotein cholesterol\n",
        "adiposity\n",
        "famhist\t\tfamily history of heart disease (Present, Absent)\n",
        "typea\t\ttype-A behavior\n",
        "obesity\n",
        "alcohol\t\tcurrent alcohol consumption\n",
        "age\t\tage at onset\n",
        "chd\t\tresponse, coronary heart disease\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.testing import assert_almost_equal\n",
        "from functools import reduce"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9YgybEa4pyup",
        "colab_type": "code",
        "outputId": "e1afca80-ea3d-4a99-f920-fc4367223d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(dat_url).drop('row.names', axis=1)\n",
        "\n",
        "assert all([x==0 for x in df.isna().sum().values])\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sbp</th>\n",
              "      <th>tobacco</th>\n",
              "      <th>ldl</th>\n",
              "      <th>adiposity</th>\n",
              "      <th>famhist</th>\n",
              "      <th>typea</th>\n",
              "      <th>obesity</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>age</th>\n",
              "      <th>chd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>160</td>\n",
              "      <td>12.00</td>\n",
              "      <td>5.73</td>\n",
              "      <td>23.11</td>\n",
              "      <td>Present</td>\n",
              "      <td>49</td>\n",
              "      <td>25.30</td>\n",
              "      <td>97.20</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>144</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.41</td>\n",
              "      <td>28.61</td>\n",
              "      <td>Absent</td>\n",
              "      <td>55</td>\n",
              "      <td>28.87</td>\n",
              "      <td>2.06</td>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>118</td>\n",
              "      <td>0.08</td>\n",
              "      <td>3.48</td>\n",
              "      <td>32.28</td>\n",
              "      <td>Present</td>\n",
              "      <td>52</td>\n",
              "      <td>29.14</td>\n",
              "      <td>3.81</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>170</td>\n",
              "      <td>7.50</td>\n",
              "      <td>6.41</td>\n",
              "      <td>38.03</td>\n",
              "      <td>Present</td>\n",
              "      <td>51</td>\n",
              "      <td>31.99</td>\n",
              "      <td>24.26</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>134</td>\n",
              "      <td>13.60</td>\n",
              "      <td>3.50</td>\n",
              "      <td>27.78</td>\n",
              "      <td>Present</td>\n",
              "      <td>60</td>\n",
              "      <td>25.99</td>\n",
              "      <td>57.34</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
              "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
              "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
              "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
              "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
              "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "3E6n9YF9p97p",
        "colab_type": "code",
        "outputId": "f018822d-71d5-4038-94f1-564b388d91ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sbp</th>\n",
              "      <th>tobacco</th>\n",
              "      <th>ldl</th>\n",
              "      <th>adiposity</th>\n",
              "      <th>typea</th>\n",
              "      <th>obesity</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>age</th>\n",
              "      <th>chd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "      <td>462.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>138.326840</td>\n",
              "      <td>3.635649</td>\n",
              "      <td>4.740325</td>\n",
              "      <td>25.406732</td>\n",
              "      <td>53.103896</td>\n",
              "      <td>26.044113</td>\n",
              "      <td>17.044394</td>\n",
              "      <td>42.816017</td>\n",
              "      <td>0.346320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>20.496317</td>\n",
              "      <td>4.593024</td>\n",
              "      <td>2.070909</td>\n",
              "      <td>7.780699</td>\n",
              "      <td>9.817534</td>\n",
              "      <td>4.213680</td>\n",
              "      <td>24.481059</td>\n",
              "      <td>14.608956</td>\n",
              "      <td>0.476313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>101.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>6.740000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>14.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>124.000000</td>\n",
              "      <td>0.052500</td>\n",
              "      <td>3.282500</td>\n",
              "      <td>19.775000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>22.985000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>134.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.340000</td>\n",
              "      <td>26.115000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>25.805000</td>\n",
              "      <td>7.510000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>148.000000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>5.790000</td>\n",
              "      <td>31.227500</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>28.497500</td>\n",
              "      <td>23.892500</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>218.000000</td>\n",
              "      <td>31.200000</td>\n",
              "      <td>15.330000</td>\n",
              "      <td>42.490000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>46.580000</td>\n",
              "      <td>147.190000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              sbp     tobacco         ldl   adiposity       typea     obesity  \\\n",
              "count  462.000000  462.000000  462.000000  462.000000  462.000000  462.000000   \n",
              "mean   138.326840    3.635649    4.740325   25.406732   53.103896   26.044113   \n",
              "std     20.496317    4.593024    2.070909    7.780699    9.817534    4.213680   \n",
              "min    101.000000    0.000000    0.980000    6.740000   13.000000   14.700000   \n",
              "25%    124.000000    0.052500    3.282500   19.775000   47.000000   22.985000   \n",
              "50%    134.000000    2.000000    4.340000   26.115000   53.000000   25.805000   \n",
              "75%    148.000000    5.500000    5.790000   31.227500   60.000000   28.497500   \n",
              "max    218.000000   31.200000   15.330000   42.490000   78.000000   46.580000   \n",
              "\n",
              "          alcohol         age         chd  \n",
              "count  462.000000  462.000000  462.000000  \n",
              "mean    17.044394   42.816017    0.346320  \n",
              "std     24.481059   14.608956    0.476313  \n",
              "min      0.000000   15.000000    0.000000  \n",
              "25%      0.510000   31.000000    0.000000  \n",
              "50%      7.510000   45.000000    0.000000  \n",
              "75%     23.892500   55.000000    1.000000  \n",
              "max    147.190000   64.000000    1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "metadata": {
        "id": "VRTLyEqQqyST",
        "colab_type": "code",
        "outputId": "f5f308f5-6218-48d2-b566-6feca56be17f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "sclr = StandardScaler()\n",
        "\n",
        "X = df.drop(['chd', 'famhist'], axis=1)\n",
        "X['famhist'] = df.famhist.map({'Present': 1, 'Absent': 0})\n",
        "X = sclr.fit_transform(X)\n",
        "y = df.chd\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
        "\n",
        "#logi_mod.classes_, logi_mod.coef_, logi_mod.intercept_\n",
        "\n",
        "#help(logi_mod)\n",
        "\n",
        "#logi_mod.predict(X)\n",
        "\n",
        "''' https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
        "\n",
        "Normally when an optimization algorithm does not converge, it is usually because the problem is not well-conditioned, perhaps due to a poor scaling of the decision variables. There are a few things you can try.\n",
        "\n",
        "    Normalize your training data so that the problem hopefully becomes more well conditioned, which in turn can speed up convergence. One possibility is to scale your data to 0 mean, unit standard deviation using Scikit-Learn's StandardScaler for an example. Note that you have to apply the StandardScaler fitted on the training data to the test data.\n",
        "    Related to 1), make sure the other arguments such as regularization weight, C, is set appropriately.\n",
        "    Set max_iter to a larger value. The default is 1000.\n",
        "\n",
        "'''\n",
        "\n",
        "logi_mod = LogisticRegression(solver='lbfgs', max_iter=1000).fit(X_train, y_train)\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "VxGkfhNSq4SJ",
        "colab_type": "code",
        "outputId": "9dc60087-55dc-4d0a-cd10-cfe0baa027f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "assert_almost_equal(1 - np.divide(abs(logi_mod.predict(X_test) - y_test).sum(), X_test.shape[0]), \n",
        "                    np.divide(sum(logi_mod.predict(X_test) == y_test), X_test.shape[0])\n",
        "                   )\n",
        "\n",
        "for l in logi_mod.predict_proba(X_test):\n",
        "  assert_almost_equal(l[0] + l[1], 1)\n",
        "  \n",
        "for l in logi_mod.predict_log_proba(X_test): \n",
        "  assert_almost_equal(np.exp(l[0])+np.exp(l[1]), 1)\n",
        "\n",
        "print('error from discretized: {}'.format(1 - np.divide(abs(logi_mod.predict(X_test) - y_test).sum(), X_test.shape[0])))\n",
        "#print('R2 (variance explained) (this is not R2/var explained): {}'.format(logi_mod.score(X_test, y_test)))\n",
        "#print('error from continuous: ' )\n",
        "predictions_proba = [l[0] for l in logi_mod.predict_proba(X_test)]\n",
        "\n",
        "#1-np.divide(sum(1-abs(y_test.values - predictions_proba)), X_test.shape[0]) ## error from smooth'd? \n",
        "\n",
        "predictions_discr = logi_mod.predict(X_test)\n",
        "\n",
        "mylist = np.array([x >= 0.5 for x in predictions_proba]).astype(int)\n",
        "\n",
        "assert_almost_equal(np.divide(sum(logi_mod.predict(X_test) == y_test), X_test.shape[0]), \n",
        "                    1- np.divide(sum(mylist == y_test), X_test.shape[0]))\n",
        "\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error from discretized: 0.646551724137931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lzXzUST7q4VR",
        "colab_type": "code",
        "outputId": "22b2c710-41d1-4be1-b3c4-b98e4f4e29c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "34km4pFZq4Pa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-qD-s0-7o1k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vdfqcN1j7pEj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "9nqpqbLp7sH3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_ = df.drop(['chd', 'famhist'], axis=1)\n",
        "X_['famhist'] = df.famhist.map({'Present': 1, 'Absent': 0})\n",
        "y_ = df.chd\n",
        "\n",
        "X_.shape, y_.shape\n",
        "\n",
        "X = pd.concat([pd.DataFrame(np.ones(X_.shape[0]), columns=['ones']), pd.DataFrame(columns = X_.columns)], axis=1) \n",
        "for feat in X_.columns: \n",
        "  X[feat] = np.divide(X_[feat] - X_[feat].mean(), X_[feat].std())\n",
        "  assert_almost_equal(X[feat].mean(), 0)\n",
        "  assert_almost_equal(X[feat].std(), 1)\n",
        "  \n",
        "X.head() # standardization\n",
        "\n",
        "beta = np.ones(X.shape[1]) # initialize, doesn't matter.\n",
        "\n",
        "def y_hat(beta, dat=X): return np.matmul(dat, beta) \n",
        "# the basic linear combination that comprises the model in Linear Regression is the input to the squisher in logistic regression\n",
        "\n",
        "def MSE(beta, dat=X): return np.divide(sum([(yy-yh)**2 for yy,yh in zip(y_,y_hat(beta, dat))]), dat.shape[0])\n",
        "# mean squared error.... hm idk if this is even relevant. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeRmM6Ox7sMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logit(p): \n",
        "  assert 0<p<1\n",
        "  return np.log(np.divide(p, 1-p))\n",
        "\n",
        "def squish(x): \n",
        "  return np.divide(1, 1+np.exp(-x)) # ok this is \n",
        "# the standard sigmoid on wikipedia but \n",
        "# Wasserman suggests that the x isn't minus. \n",
        "\n",
        "assert_almost_equal(logit(squish(np.exp(-2))), np.exp(-2))\n",
        "assert_almost_equal(logit(squish(np.exp(2))), np.exp(2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0xGrzMA1GZ7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{data}$ is a dataframe, all numeric and has been cleaned of nulls. Slice off one of it's features: this is our _dependent variable_. we have $N-1$ _observations_ and $M$ _independent features_\n",
        "\n",
        "Our **dependent variable** is just 0 or 1. Two _classes_. \n",
        "\n",
        "we operate on that until we have $\\textbf{X}$, a **dataframe of independent variables after standardization** of size $(N \\times M)$,  a **column of ones** has been appended to the _leftmost_ side. \n",
        "\n",
        "The probability that observation $i$ is a member of class `1` (i.e. \"has heart disease\") is: $p = sigmoid(\\textbf{X}\\beta)$ where $\\beta = (\\beta_0, \\beta_1, ... , \\beta_n)$ and $i$ is in $(1, ... , n)$\n",
        "\n",
        "Apply $logit$ to both sides, $ logit(p) = logit(sigmoid(\\textbf{X}\\beta))$, noticing that $logit(sigmoid(x))==x$, so your final equation is $logit(p) = \\textbf{X} \\beta $\n",
        "\n",
        "where $p$ is a column of probabilities, equal in length to the number of observations. Each component of $p$ is the probability that it's corresponding observation has heart disease. I.e. if row $i$ of our data (before the test data $y$ had been sliced off, the standardization was applied, and the ones column was added) represents a person, we say \"I am $p_i$ percent sure that person #$i$ has heart disease\" (for any $i$ in $(0,..,n)$)"
      ]
    },
    {
      "metadata": {
        "id": "lpVtDncvPwLo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The _conditional likelihood_ is $\\mathcal{L}(\\beta) = \\Pi_{i=1}^{n}(p_i(\\beta)^{y_i}(1-p_i(\\beta))^{1-y_i})$ (remember: $y_i$'s values are entirely in ${0,1}$!). Since we **prefer addition ${and}$ $\\log$ is _monotonic_** (order preserving), we convert it to $\\log(\\mathcal{L}(\\beta)) = \\mathcal{l}(\\beta) = \\Sigma_{i=1}^{n}\\log(p_i(\\beta)^{y_i}(1-p_i(\\beta))^{1-y_i})$. In other words, once you have probabilities of heart disease for each observation, simply add the $\\log$ of the \"has heart disease\" probability if the _test data_ says \"person #i has heart disease\", and if the test data says \"person #i doesn't have heart disease\" then add the $\\log$ of the negated probability, $1-p_i$.  "
      ]
    },
    {
      "metadata": {
        "id": "4wzn9PkcK9j2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The goal then is to find the $\\beta$ that minimizes error, which is similar to maximizing the likelihood $(P(\\textbf{data} | \\beta)) = \\mathcal{L}(\\beta)) $"
      ]
    },
    {
      "metadata": {
        "id": "7yxC8Acm7sPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def p(beta, dat=X): \n",
        "  return squish(y_hat(beta, dat))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2xjePJ5i0H1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## luckily for us, this problem has been _analytically understood_, meaning there's nothing going on but an equation i can write down, provided i have the resources to matrically multiply. it's called the **reweighted least squares algorithm**\n",
        "\n",
        "The equation is $\\beta = (\\textbf{X}^T\\textbf{WX})^{-1}\\textbf{X}^T\\textbf{Wy}  $, where $\\textbf{y}$ is training data and $\\textbf{W}$ is a function of $\\beta$"
      ]
    },
    {
      "metadata": {
        "id": "W0NI8Jmo7sKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2032
        },
        "outputId": "1cdfcc80-8473-445a-bb36-18fb664ade7a"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y_)\n",
        "\n",
        "#\n",
        "def update(beta, dat=X_train, depend=y_train): \n",
        "  pks = p(beta, dat)\n",
        "  W = np.diag(pks*(1-pks))\n",
        "\n",
        "  fact0 = np.matmul(dat.T, W)\n",
        "  fact1 = np.matmul(fact0, dat)\n",
        "  fact2 = np.linalg.inv(fact1)\n",
        "  fact3 = np.matmul(fact2, fact0)\n",
        "  return np.matmul(fact3, depend)\n",
        "\n",
        "\n",
        "def repeatedly(f, n): \n",
        "  def composition(f,g):\n",
        "    # \"f after g\" \n",
        "    return lambda x: f(g(x))\n",
        "  return reduce(lambda ma, mma: composition(ma, mma), [f]*n)\n",
        "\n",
        "\n",
        "\n",
        "update25 = repeatedly(update, 25)\n",
        "\n",
        "update25(beta)\n",
        "\n",
        "#MSE(beta_hat, X_train)\n",
        "\n",
        "p(update25(beta))\n",
        "# hrm, looking at these they're all wishy-washy! like it's madly uncertain! \n",
        "\n",
        "(p(update25(beta)) < 0.37).any() or (p(update25(beta)) > 0.73).any() # ugh\n",
        "\n",
        "# logi_mod.predict_proba(X_test.drop('ones', axis=1))\n"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.59941376, 0.40058624],\n",
              "       [0.80303276, 0.19696724],\n",
              "       [0.96079411, 0.03920589],\n",
              "       [0.55555591, 0.44444409],\n",
              "       [0.68295156, 0.31704844],\n",
              "       [0.40474347, 0.59525653],\n",
              "       [0.20526323, 0.79473677],\n",
              "       [0.28438035, 0.71561965],\n",
              "       [0.30018286, 0.69981714],\n",
              "       [0.1583292 , 0.8416708 ],\n",
              "       [0.83559228, 0.16440772],\n",
              "       [0.35497416, 0.64502584],\n",
              "       [0.74637724, 0.25362276],\n",
              "       [0.57404503, 0.42595497],\n",
              "       [0.83081611, 0.16918389],\n",
              "       [0.22985571, 0.77014429],\n",
              "       [0.97408342, 0.02591658],\n",
              "       [0.44937014, 0.55062986],\n",
              "       [0.80031558, 0.19968442],\n",
              "       [0.36619583, 0.63380417],\n",
              "       [0.32508194, 0.67491806],\n",
              "       [0.24087165, 0.75912835],\n",
              "       [0.77027345, 0.22972655],\n",
              "       [0.89297523, 0.10702477],\n",
              "       [0.7646139 , 0.2353861 ],\n",
              "       [0.74853704, 0.25146296],\n",
              "       [0.63092739, 0.36907261],\n",
              "       [0.7212911 , 0.2787089 ],\n",
              "       [0.5815017 , 0.4184983 ],\n",
              "       [0.97638393, 0.02361607],\n",
              "       [0.37020766, 0.62979234],\n",
              "       [0.73419953, 0.26580047],\n",
              "       [0.85868064, 0.14131936],\n",
              "       [0.78659838, 0.21340162],\n",
              "       [0.95123572, 0.04876428],\n",
              "       [0.69488989, 0.30511011],\n",
              "       [0.90204564, 0.09795436],\n",
              "       [0.2645185 , 0.7354815 ],\n",
              "       [0.81623646, 0.18376354],\n",
              "       [0.58221557, 0.41778443],\n",
              "       [0.61728436, 0.38271564],\n",
              "       [0.64319569, 0.35680431],\n",
              "       [0.81897828, 0.18102172],\n",
              "       [0.99556726, 0.00443274],\n",
              "       [0.98717127, 0.01282873],\n",
              "       [0.70850942, 0.29149058],\n",
              "       [0.44890204, 0.55109796],\n",
              "       [0.94987253, 0.05012747],\n",
              "       [0.83627317, 0.16372683],\n",
              "       [0.52439855, 0.47560145],\n",
              "       [0.953883  , 0.046117  ],\n",
              "       [0.90332988, 0.09667012],\n",
              "       [0.95224378, 0.04775622],\n",
              "       [0.93580546, 0.06419454],\n",
              "       [0.41998305, 0.58001695],\n",
              "       [0.84577901, 0.15422099],\n",
              "       [0.67089843, 0.32910157],\n",
              "       [0.75692107, 0.24307893],\n",
              "       [0.86681978, 0.13318022],\n",
              "       [0.71881181, 0.28118819],\n",
              "       [0.97273826, 0.02726174],\n",
              "       [0.1579881 , 0.8420119 ],\n",
              "       [0.54960972, 0.45039028],\n",
              "       [0.728854  , 0.271146  ],\n",
              "       [0.96963801, 0.03036199],\n",
              "       [0.95521453, 0.04478547],\n",
              "       [0.8931638 , 0.1068362 ],\n",
              "       [0.98202184, 0.01797816],\n",
              "       [0.81603172, 0.18396828],\n",
              "       [0.05684515, 0.94315485],\n",
              "       [0.94422896, 0.05577104],\n",
              "       [0.89136866, 0.10863134],\n",
              "       [0.16437202, 0.83562798],\n",
              "       [0.79313624, 0.20686376],\n",
              "       [0.50203568, 0.49796432],\n",
              "       [0.84198709, 0.15801291],\n",
              "       [0.58333018, 0.41666982],\n",
              "       [0.89910305, 0.10089695],\n",
              "       [0.77608336, 0.22391664],\n",
              "       [0.69468345, 0.30531655],\n",
              "       [0.40907373, 0.59092627],\n",
              "       [0.82759502, 0.17240498],\n",
              "       [0.97068439, 0.02931561],\n",
              "       [0.70918192, 0.29081808],\n",
              "       [0.69543052, 0.30456948],\n",
              "       [0.58977095, 0.41022905],\n",
              "       [0.9663288 , 0.0336712 ],\n",
              "       [0.78466507, 0.21533493],\n",
              "       [0.24864946, 0.75135054],\n",
              "       [0.6728008 , 0.3271992 ],\n",
              "       [0.97961246, 0.02038754],\n",
              "       [0.16937344, 0.83062656],\n",
              "       [0.18281071, 0.81718929],\n",
              "       [0.65770035, 0.34229965],\n",
              "       [0.64608944, 0.35391056],\n",
              "       [0.35156434, 0.64843566],\n",
              "       [0.59153117, 0.40846883],\n",
              "       [0.97079908, 0.02920092],\n",
              "       [0.72496601, 0.27503399],\n",
              "       [0.47615929, 0.52384071],\n",
              "       [0.19958484, 0.80041516],\n",
              "       [0.85281509, 0.14718491],\n",
              "       [0.88099716, 0.11900284],\n",
              "       [0.70229289, 0.29770711],\n",
              "       [0.85409751, 0.14590249],\n",
              "       [0.70789228, 0.29210772],\n",
              "       [0.72205825, 0.27794175],\n",
              "       [0.52960667, 0.47039333],\n",
              "       [0.61251902, 0.38748098],\n",
              "       [0.91451156, 0.08548844],\n",
              "       [0.68319674, 0.31680326],\n",
              "       [0.5841929 , 0.4158071 ],\n",
              "       [0.62221195, 0.37778805],\n",
              "       [0.20859668, 0.79140332],\n",
              "       [0.21337242, 0.78662758],\n",
              "       [0.82530206, 0.17469794]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "metadata": {
        "id": "DK1omHzGodlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b733042-7c9d-4230-fcc4-325e7dfad645"
      },
      "cell_type": "code",
      "source": [
        "# discretize the predictions with 0.5\n",
        "my_discretized_predictions = np.array([x >= 0.5 for x in p(beta_hat, X_test)]).astype(int)\n",
        "\n",
        "# assert_almost_equal(np.divide(sum(logi_mod.predict(X_test) == y_test), X_test.shape[0]), \n",
        "#                     1- np.divide(sum(mylist == y_test), X_test.shape[0]))\n",
        "\n",
        "my_discretized_predictions.shape, y.shape\n",
        "# assert_almost_equal(np.divide(sum(my_discretized_predictions==y_), X_.shape[0]), \n",
        "#                     1-np.divide(sum(my_discretized_predictions == y_), X_.shape[0]))\n",
        "\n",
        "\n",
        "np.divide(sum(my_discretized_predictions == y_test), y_test.shape[0])\n",
        "\n",
        "\n"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5517241379310345"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "metadata": {
        "id": "hAwTT9JyusxD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_definition_\n",
        "\n",
        "$\\textbf{f is monotonic} \\iff x \\leq y \\implies f x \\leq f y$ \n",
        "\n",
        "whenever $f$'s domain and codomain each have partial orders. "
      ]
    }
  ]
}