{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Josh Mancuso DS3 Sprint13 Challenge Parts 1 & 2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A_dVoLwp3Lv",
        "colab_type": "text"
      },
      "source": [
        "#Part 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct4QHM9do9bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask.dataframe as dd\n",
        "from dask import compute"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZwopX4mo9by",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "258a57c1-dad1-420a-88e1-a96dac3b314d"
      },
      "source": [
        "#Load the five csv files into one Dask Dataframe. It should have a length of 1956 rows, and 5 columns.\n",
        "df = dd.read_csv('Youtube*.csv')\n",
        "compute(df.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1956, 5),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNd8s7gjo9b5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "a070f793-69a6-401e-ec56-e3f44db6c6f2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COMMENT_ID</th>\n",
              "      <th>AUTHOR</th>\n",
              "      <th>DATE</th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>CLASS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
              "      <td>Julius NM</td>\n",
              "      <td>2013-11-07T06:20:48</td>\n",
              "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
              "      <td>adam riyati</td>\n",
              "      <td>2013-11-07T12:37:15</td>\n",
              "      <td>Hey guys check out my new channel and our firs...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
              "      <td>Evgeny Murashkin</td>\n",
              "      <td>2013-11-08T17:34:21</td>\n",
              "      <td>just for test I have to say murdev.com</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
              "      <td>ElNino Melendez</td>\n",
              "      <td>2013-11-09T08:28:43</td>\n",
              "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
              "      <td>GsMega</td>\n",
              "      <td>2013-11-10T16:05:38</td>\n",
              "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    COMMENT_ID  ... CLASS\n",
              "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU  ...     1\n",
              "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A  ...     1\n",
              "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  ...     1\n",
              "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04  ...     1\n",
              "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZsCD8Nro9cC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1c2ad1de-3f22-4463-9de5-801329b54da7"
      },
      "source": [
        "#Use the Dask Dataframe to compute the counts of spam (1005 comments) versus the counts of legitimate comments (951).\n",
        "compute(df.CLASS.value_counts())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1    1005\n",
              " 0     951\n",
              " Name: CLASS, dtype: int64,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clTIyZL-o9cJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "b5973244-9e02-4803-cc10-ea608179e3ce"
      },
      "source": [
        "'''Spammers often tell people to check out their stuff! When the comments are converted to lowercase, then 461 spam comments \n",
        "contain the word \"check\", versus only 19 legitimate comments which contain the word \"check.\" Use the Dask Dataframe to \n",
        "compute these counts.'''\n",
        "\n",
        "#lowercase the content column:\n",
        "df.CONTENT = df.CONTENT.str.lower()\n",
        "\n",
        "#separate spam/legit comments:\n",
        "spam = df[df.CLASS == 1]\n",
        "legit = df[df.CLASS == 0]\n",
        "\n",
        "#spam comments that contain the string 'check':\n",
        "print('Number of spam comments containing the string \"check\": ')\n",
        "print(len(spam[spam.CONTENT.str.contains(\"check\") == True].compute()))\n",
        "\n",
        "#legit comments that contain the string 'check':\n",
        "print('Number of legitimate comments containing the string \"check\": ')\n",
        "print(len(legit[legit.CONTENT.str.contains(\"check\") == True].compute()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of spam comments containing the string \"check\": \n",
            "461\n",
            "Number of legitimate comments containing the string \"check\": \n",
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x94uyKR6AZjP",
        "colab_type": "text"
      },
      "source": [
        "# Part 2. Big data options\n",
        "You've been introduced to a variety of platforms (AWS SageMaker, AWS EMR, Databricks), libraries (Numba, Dask, MapReduce, Spark), and languages (Python, SQL, Scala, Java) that can \"scale up\" or \"scale out\" for faster processing of big data.\n",
        "\n",
        "Write a paragraph comparing some of these technology options. For example, you could describe which technology you may personally prefer to use, in what circumstances, for what reasons.\n",
        "\n",
        "(You can add your paragraph as a Markdown cell at the bottom of your SageMaker Notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzyLy_Cxp78b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AWS EMR vs AWS Sagemaker vs Databricks:\n",
        "  EMR is a managed cluster platform to simplify running big data frameworks like\n",
        "  Spark on AWS, and in general, to handle vast amounts of data.\n",
        "  SageMaker is a platform for developers and data scientis to easily build and \n",
        "  deploy machine learning models at a bigger scale. Databricks seems similar to \n",
        "  Sagemaker, but I find the interface a bit easier to navigate as a newb to using \n",
        "  cloud based platforms.\n",
        "  \n",
        "Numba/Dask/Spark:\n",
        "  Numba and Dask are pretty useful tools in the Data Scientist toolkit. They basically\n",
        "  serve to augment Pandas and Numpy, specifically to speed up the underlying \n",
        "  computations of dataframes and numpy arrays by bypassing python compilation,\n",
        "  since abstraction of python is slow for large datasets at scale. They also improve \n",
        "  memory usage and performannce by delaying execution of commands/queries until\n",
        "  necessary. Spark also is useful at improving speed/efficiency of big data, and\n",
        "  seems to combine/integrate Scala/SQL/Python/Java.\n",
        "\n",
        "Languages (python / SQL/ Scala):\n",
        "  While Python seems like the easiest to use major programming language overall,\n",
        "  SQL is simple to learn and still relevant for querying relational databases. Scala\n",
        "  has many similarities to Python, and while I am completely unfamiliar with Java,\n",
        "  I think that Scala has is a sort of hybrid of Java/Python. "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}