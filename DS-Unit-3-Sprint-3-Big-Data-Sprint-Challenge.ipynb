{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distibuted client for parallel computation\n",
    "client = Client(n_workers=16) # Since we are using AWS SageMaker with 16 cores\n",
    "\n",
    "youtube_df = dd.read_csv('/home/ec2-user/SageMaker/Youtube*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in dataframe is 5\n",
      "Number of rows in dataframe is 1956\n"
     ]
    }
   ],
   "source": [
    "# Check the number of columns and rows in the dataframe\n",
    "print(f\"Number of columns in dataframe is {len(youtube_df.columns)}\")\n",
    "print(f\"Number of rows in dataframe is {len(youtube_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam count is 1005 and legitimate comments count is 951\n"
     ]
    }
   ],
   "source": [
    "(spam, notspam) = youtube_df.CLASS.value_counts().compute()\n",
    "print(f\"spam count is {spam} and legitimate comments count is {notspam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating out data with respect to spam and nonspam\n",
    "spamdf = youtube_df[youtube_df.CLASS == 1].compute()\n",
    "nonspamdf = youtube_df[youtube_df.CLASS == 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments containing 'check' in spams is 461\n",
      "Number of comments containing 'check' in spams is 19\n"
     ]
    }
   ],
   "source": [
    "# Computing the count of 'check' in spam and nonspam data\n",
    "checkwordcount_spam = 0\n",
    "for comment in spamdf.CONTENT:\n",
    "    if \"check\" in comment.lower():\n",
    "        checkwordcount_spam += 1\n",
    "\n",
    "print(f\"Number of comments containing 'check' in spams is {checkwordcount_spam}\")\n",
    "\n",
    "checkwordcount_nonspam = 0\n",
    "for comment in nonspamdf.CONTENT:\n",
    "    if \"check\" in comment.lower():\n",
    "        checkwordcount_nonspam += 1\n",
    "\n",
    "print(f\"Number of comments containing 'check' in spams is {checkwordcount_nonspam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To score a 3, do extra work, such as creating the Dask Distributed Client, or creating a visualization with this dataset.\n",
    "\n",
    "# I have added below code to complete this stretch goal already.\n",
    "# client = Client(n_workers=16) # Since we are using AWS SageMaker with 16 cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big data options\n",
    "\n",
    "I would like to first discuss about the platform.\n",
    "\n",
    "**AWS Sagemaker:**\n",
    "\n",
    "AWS Sagemaker is a powerful platform for working with big dataset which needs very high computational resources (CPU and RAM to be specific). Our exisiting PCs/laptops have limitation with respect to computational resource. For example my laptop has 8 core CPU and 32 GB RAM capacity. If the data which needs to be processed is huge and takes lot of time to process assume 10 GB of data, then the capacity on my existing laptop is not sufficient. (As per guidelines usually 10X amount of capacity is needed).\n",
    "\n",
    "In this case, I can make use of AWS Sagemaker which gives me an option to **Scale-Up** my capacity and work with huge data sets. Advantage of this approach is that there is no need for actually worrying about partitioning the data and combining it together later since we are running on a single system. \n",
    "\n",
    "**AWS EMR/Databricks**\n",
    "\n",
    "As an alternative to **Scale-Up** which usually needs bigger and powerful computational resource on a single node, we can use the approach of **Scale-Out** using similar or almost similar nodes with comparable computational resources. The major disadvantage of **Scale-Up** approach is there is a physical and economical limitation with respect to how much we can Scale-Up computationally. In simpler words, there are limitation on how big the Super Computers can be and also they are very very expensive.\n",
    "\n",
    "So with **Scale-Out** approach, we can have nodes having a lower computational resources but use them together is a managed way to distribute the work among them logically and process the data. The technology required for such distributed computing (like Hadoop, Spark...) are existing and can be used. So here instead of going for expensive AWS Sagemaker, we can use nodes of lower capacity like AWS EMR/Databricks. Some key disadvantage of this approch is fault tolerance/reducancy needs to be considered. Also for distributing the computation and managing the task their would be additional overhead.\n",
    "\n",
    "\n",
    "Next I would like to first discuss about the libraries.\n",
    "\n",
    "**Numba**\n",
    "\n",
    "Numba is a just-in-time compiler for Python that works best on code that uses NumPy arrays and functions, and loops. Numba library optimises the execution of instructions using just-in-time compiling and can work with existing python code.\n",
    "\n",
    "However Numba is suited best to work with Numpy based code and has limitation with respect to other libraries like Pandas.\n",
    "Ideal for initial step of **Step-Up** optimization.\n",
    "\n",
    "**Dask**\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python.\n",
    "\n",
    "Dask is composed of two parts:\n",
    "\n",
    "1. Dynamic task scheduling optimized for computation and optimized for interactive computational workloads.\n",
    "2 . “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.\n",
    "\n",
    "Ideal for **Step-Up** optimization and also for **Step-Out** approaches where we can distribute the load across multiple nodes. However, for better **Step-Out** optimization MapReduce/Spark would be better alternative.\n",
    "\n",
    "The interface are very similar to Pandas and Sci-kit learn and needs minimal code changes.\n",
    "\n",
    "**MapReduce/Spark**\n",
    "\n",
    "As mentioned above, both MapReduce/Spark are developed to support **Step-Out** based computation and is best suited for Big Data processing which usually spans across multiple noded. The underlying computation overhead like distribution of data, fault tolerance, redundacy and consolidation is handled by these libraries.\n",
    "\n",
    "To help development many of the common constructs from exisiting technology are supported on these libraries. For example Spark supports development on Scala, Python and Java. It also supports programming constructs similar to DataFrame and SQL.\n",
    "\n",
    "Lastly let us consider the languages.\n",
    "\n",
    "**Python**\n",
    "\n",
    "Python is multipurpose programming langage which is extensively used in Data Science also. When considering **Scale-Up or Scale-Out** approaches Python has good libraries developed for both cases.\n",
    "\n",
    "Numba and Dask can be used to optimize the python code on single nodes with just-in-time compilation and parallely processing.\n",
    "\n",
    "Similarly, technology like Spark provides interface for developing code in Python and running ontop of Spark (PySpark) which is not very much optimized as compared to Scala Spark.\n",
    "\n",
    "**SQL**\n",
    "\n",
    "SQL is similar to Python. Spark supports interaction with data using SQL queries which is a very convinent way. In fact, native DataFrame or SQL based approach are evaluated by Spark platform similarly.\n",
    "\n",
    "**Scala**\n",
    "\n",
    "Scala is the 1st citizen in the world of Spark and in fact Spart is developed using Scala. So many of the interfaces in Spark are optimized for Scala language. Scala supports functional programming approch which is very useful for **Scale-Out** based approach.\n",
    "\n",
    "**Java**\n",
    "\n",
    "Java support on Spark is similar to Python.\n",
    "\n",
    "**My Preference**\n",
    "\n",
    "I would initially use python (Numpy/Pandas/Sci-Kit Learn) on single node system for small/medium data set. If the computation resource needed increases to a large data set, I would optimize it python code using Numba or Dask. Finally incase of Scale-Out is needed, I would develop the code using Scala/PySpark on Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
