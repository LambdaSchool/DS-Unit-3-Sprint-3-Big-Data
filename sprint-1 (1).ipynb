{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next time should use: \n",
    "\n",
    "df = dd.read_csv('*.csv')\n",
    "df[\"CLASS\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CONTENT'] = df['CONTENT'].astype(str).str.lower()\n",
    "df['CONTENT'][df[\"CLASS\"]==0].str.contains(\"check\").value_counts().compute().loc[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CONTENT'][df[\"CLASS\"]==1].str.contains(\"check\").value_counts().compute().loc[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy = dd.read_csv(\"Youtube01-Psy.csv\")\n",
    "katy = dd.read_csv(\"Youtube02-KatyPerry.csv\")\n",
    "LMFAO = dd.read_csv(\"Youtube03-LMFAO.csv\")\n",
    "eminem = dd.read_csv(\"Youtube04-Eminem.csv\")\n",
    "shakira = dd.read_csv(\"Youtube05-Shakira.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam content: 1005\n"
     ]
    }
   ],
   "source": [
    "a = psy[\"CLASS\"].value_counts().compute().loc[1]\n",
    "b = katy[\"CLASS\"].value_counts().compute().loc[1]\n",
    "c = LMFAO[\"CLASS\"].value_counts().compute().loc[1]\n",
    "d = eminem[\"CLASS\"].value_counts().compute().loc[1]\n",
    "e = shakira[\"CLASS\"].value_counts().compute().loc[1]\n",
    "print('spam content:', a + b + c + d + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legitimate content: 951\n"
     ]
    }
   ],
   "source": [
    "a2 = psy[\"CLASS\"].value_counts().compute().loc[0]\n",
    "b2 = katy[\"CLASS\"].value_counts().compute().loc[0]\n",
    "c2 = LMFAO[\"CLASS\"].value_counts().compute().loc[0]\n",
    "d2 = eminem[\"CLASS\"].value_counts().compute().loc[0]\n",
    "e2 = shakira[\"CLASS\"].value_counts().compute().loc[0]\n",
    "print('legitimate content:', a2+b2+c2+d2+e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting object to string and then strings to lowercase \n",
    "\n",
    "psy['CONTENT'] = psy['CONTENT'].astype(str).str.lower()\n",
    "katy['CONTENT'] = katy['CONTENT'].astype(str).str.lower()\n",
    "LMFAO['CONTENT'] = LMFAO['CONTENT'].astype(str).str.lower()\n",
    "eminem['CONTENT'] = eminem['CONTENT'].astype(str).str.lower()\n",
    "shakira['CONTENT'] = shakira['CONTENT'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legitimate comments that contain the target string: 19\n"
     ]
    }
   ],
   "source": [
    "aa = psy['CONTENT'][psy[\"CLASS\"]==0].str.contains(\"check\").value_counts().compute().loc[False]\n",
    "bb = katy['CONTENT'][katy[\"CLASS\"]==0].str.contains(\"check\").value_counts().compute().loc[False]\n",
    "cc = LMFAO['CONTENT'][LMFAO[\"CLASS\"]==0].str.contains(\"check\").value_counts().compute().loc[False]\n",
    "dd = eminem['CONTENT'][eminem[\"CLASS\"]==0].str.contains(\"check\").value_counts().compute().loc[False]\n",
    "ee = shakira['CONTENT'][shakira[\"CLASS\"]==0].str.contains(\"check\").value_counts().compute().loc[False]\n",
    "\n",
    "aa+bb+cc+dd+ee\n",
    "\n",
    "print(\"legitimate comments that contain the target string:\", (a2+b2+c2+d2+e2)-(aa+bb+cc+dd+ee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam comments that contain the target string: 461\n"
     ]
    }
   ],
   "source": [
    "p = psy['CONTENT'][psy[\"CLASS\"]==1].str.contains(\"check\").value_counts().compute().loc[True]\n",
    "k = katy['CONTENT'][katy[\"CLASS\"]==1].str.contains(\"check\").value_counts().compute().loc[True]\n",
    "l = LMFAO['CONTENT'][LMFAO[\"CLASS\"]==1].str.contains(\"check\").value_counts().compute().loc[True]\n",
    "e = eminem['CONTENT'][eminem[\"CLASS\"]==1].str.contains(\"check\").value_counts().compute().loc[True]\n",
    "s = shakira['CONTENT'][shakira[\"CLASS\"]==1].str.contains(\"check\").value_counts().compute().loc[True]\n",
    "\n",
    "p+k+l+e+s\n",
    "\n",
    "print(\"spam comments that contain the target string:\", p+k+l+e+s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2: \n",
    "    \n",
    "Regarding the different technologies we have been using this week, it has been interesting to see the sheer amount \n",
    "of available tools out there; Databricks provided to be very convenient to learn Scala, as its interface \n",
    "is very clean and easy to use. Creating clusters on Databricks is fast and differently from AWS, the community \n",
    "side of it is free. AWS's Sagemaker allowed us to create instances that run a more powerful version of Jupyter, enabling\n",
    "us to use a very familiar tool in a way that is more conducive to Big Data. Running instances with 4x cores on the cloud \n",
    "is powerful and AWS's 1500 dollars in credits will definitelly go a long way. \n",
    "\n",
    "Scala is definitely a handful, it provides to be easier for coders already familiar with the Java language, nevertheless, \n",
    "it was interesting to learn the basics. Using SQL in Spark was also very practical, not only solidifying some of our prior \n",
    "SQL knowledge but also adding some handy Spark/SQL integrations. While Dask could be useful, I still have not used it in a way where Pandas would not have sufficed; likewise, with Numba, although allowing us to run some python functions more efficiently, I still have not found a practical usecase for it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
