{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and read `csv`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask import compute, delayed\n",
    "\n",
    "comments = dd.read_csv('*.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    COMMENT_ID            AUTHOR  \\\n",
      "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
      "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
      "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
      "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
      "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
      "\n",
      "                  DATE                                            CONTENT  \\\n",
      "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
      "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
      "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
      "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
      "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
      "\n",
      "   CLASS  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check size:\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check features\n",
    "len(comments.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    1005\n",
       " 0     951\n",
       " Name: CLASS, dtype: int64,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check counts of spam (1) and non-spam (0)\n",
    "\n",
    "compute(comments.CLASS.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the word \"check\"\n",
    "\n",
    "comments['has_check'] = comments['CONTENT'].str.lower().str.contains('check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False    932\n",
      "True      19\n",
      "Name: has_check, dtype: int64,)\n",
      "(False    544\n",
      "True     461\n",
      "Name: has_check, dtype: int64,)\n"
     ]
    }
   ],
   "source": [
    "# Non-spam comments with \"check\"\n",
    "print(compute(comments[comments['CLASS']==0]['has_check'].value_counts()))\n",
    "\n",
    "# Spam comments with \"check\"\n",
    "print(compute(comments[comments['CLASS']==1]['has_check'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different problems call for different solutions. Generally speaking, Amazon SageMaker is a better solution of training machine learning models--they're what it was designed for. By contrast, EMR is more suitable for extremely large-scale data processing and analytics (so it would be more appropriate for storing and analyzing large databases, for example).\n",
    "\n",
    "But using these solutions can be overkill. For small-scale testing and exploration, using pandas and Colab (provided data security is not essential) is good enough to start with. Dask and Numba are great for performing lots of operations very quickly, but only in circumstances in which they're truly called for; otherwise, the additional setup time ends up outweighing the time saved by better optimized processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
