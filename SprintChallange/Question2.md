The variaty of tools we have been given this week, I have my favorites, but I will start with some summaries of what each one is.

- Python
    is general purpose tool for working with the data, exploring and is usable for most everything. General all around tool.

- sql
  SQL is for DataBase queries, SQL also has the ability to do much much more though, although for most cases it is generally a DB Query language.

-scala
  Scala, is actually my prefered language, I grew up learning C, and C++. I worked some with C# and Java, but Scala's syntax is so much closer to what I am used to, It is much prefered to me. Scala was built to scale as problems expanded hence where the name came from Scalable Language. I also love the fact that scala has a REPL, and a compiler!!!

- Java
    We haven't touched much on Java other than it is the base VM for Scala.

- SageMaker is basically Jupyter notebooks at Scale. AWS setup sagemaker, and it gives access to a large machine for whatever time is needed to do the job.

- DataBricks, is an environment that uses Spark, it uses Apache Zepplin(which I am not going to go into, but I want installed on my personal Server!)

- Numba is a parallel acceleration library that uses JIT compiling to accelerate the execution speed of certain operations, and numpy code.

- Dask is similar to Pandas, but it doesn't keep the dataframes in memory to allow for larger datasets to be used with lower system resources.

- MapReduce Not going to go into this one, as it is not used as much today, basically it is a workflow that was used and started the revolution to make things easier and better.

- Spark is an Apache Distributed capable framework for the processing of large scale Datasets with executors that a compute central node controls and dispatches for execution on hardware that can be distributed throughout a server farm.
