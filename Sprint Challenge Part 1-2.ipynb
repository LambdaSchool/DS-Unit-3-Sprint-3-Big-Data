{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. SageMaker & Dask\n",
    "\n",
    "In this part, you'll work with a dataset of [YouTube Spam Comments](https://christophm.github.io/interpretable-ml-book/spam-data.html).\n",
    "\n",
    "> We work with 1956 comments from 5 different YouTube videos. The comments were collected via the YouTube API from five of the ten most viewed videos on YouTube in the first half of 2015. All 5 are music videos. One of them is “Gangnam Style” by Korean artist Psy. The other artists were Katy Perry, LMFAO, Eminem, and Shakira.\n",
    "\n",
    "> The comments were manually labeled as spam or legitimate. Spam was coded with a “1” and legitimate comments with a “0”.\n",
    "\n",
    "Start an Amazon SageMaker Notebook instance. (Any instance type is ok. This can take a few minutes.) Open Jupyter. \n",
    "\n",
    "### Terminal\n",
    "In the Jupyter dashboard, choose **New**, and then choose **Terminal.**\n",
    "\n",
    "Run these commands in the terminal:\n",
    "\n",
    "1. Upgrade Dask in the conda environment named python3. (This command upgrades Bokeh too, even though you don't need to use it, because the packages seem to have dependencies. This can take a few minutes.)\n",
    "```\n",
    "conda install -n python3 bokeh dask\n",
    "```\n",
    "\n",
    "2. Change directory to SageMaker\n",
    "```\n",
    "cd SageMaker\n",
    "```\n",
    "\n",
    "3. Download data\n",
    "```\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
    "```\n",
    "\n",
    "4. Unzip data\n",
    "```\n",
    "unzip YouTube-Spam-Collection-v1.zip\n",
    "```\n",
    "\n",
    "5. See there are five csv files\n",
    "```\n",
    "ls *.csv\n",
    "```\n",
    "\n",
    "Then you can close the terminal window. \n",
    "\n",
    "### Notebook\n",
    "Create a new notebook, with the **conda_python3** kernel.\n",
    "\n",
    "For this Sprint Challenge, you *don't* need to create a Dask Distributed Client. You can just use a Dask Dataframe.\n",
    "\n",
    "Load the five csv files into one Dask Dataframe. It should have a length of 1956 rows, and 5 columns.\n",
    "\n",
    "Use the Dask Dataframe to compute the counts of spam (1005 comments) versus the counts of legitimate comments (951).\n",
    "\n",
    "Spammers often tell people to check out their stuff! When the comments are converted to lowercase, then 461 spam comments contain the word \"check\", versus only 19 legitimate comments which contain the word \"check.\" Use the Dask Dataframe to compute these counts.\n",
    "\n",
    "### Optional bonus\n",
    "To score a 3, do extra work, such as creating the Dask Distributed Client, or creating a visualization with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/bokeh/core.py:57: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn('\\n' + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33335\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:34047/status' target='_blank'>http://127.0.0.1:34047/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>16</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>33.10 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:33335' processes=16 cores=16>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import compute, delayed\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=16)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ec2-user ec2-user 57K Mar 26  2017 Youtube01-Psy.csv\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user 63K Mar 26  2017 Youtube02-KatyPerry.csv\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user 63K Mar 26  2017 Youtube03-LMFAO.csv\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user 81K Mar 26  2017 Youtube04-Eminem.csv\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user 72K Mar 26  2017 Youtube05-Shakira.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure all 5 csv files are ready\n",
    "%ls -lh *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 12.4 µs\n"
     ]
    }
   ],
   "source": [
    "# load all csvs into a dask dataframe\n",
    "%time\n",
    "yt_spam = dd.read_csv('Youtube*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=5</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 15 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              COMMENT_ID  AUTHOR    DATE CONTENT  CLASS\n",
       "npartitions=5                                          \n",
       "                  object  object  object  object  int64\n",
       "                     ...     ...     ...     ...    ...\n",
       "...                  ...     ...     ...     ...    ...\n",
       "                     ...     ...     ...     ...    ...\n",
       "                     ...     ...     ...     ...    ...\n",
       "Dask Name: from-delayed, 15 tasks"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check values of spam or legitimate\n",
    "yt_spam['CLASS'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column for checking lower case 'check'\n",
    "yt_spam['check'] = yt_spam['CONTENT'].str.lower().str.contains('check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate btwn spam check and legitimate check\n",
    "spam = yt_spam[yt_spam['CLASS'] == 1]\n",
    "not_spam = yt_spam[yt_spam['CLASS'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/dask/core.py:119: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return func(*args2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check against what is classified as spam\n",
    "check_spam = spam[yt_spam['check']]\n",
    "len(check_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/dask/core.py:119: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return func(*args2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check against what is classified as legitimate\n",
    "check_not_spam = not_spam[yt_spam['check']]\n",
    "len(check_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Big data options\n",
    "You've been introduced to a variety of platforms (AWS SageMaker, AWS EMR, Databricks), libraries (Numba, Dask, MapReduce, Spark), and languages (Python, SQL, Scala, Java) that can \"scale up\" or \"scale out\" for faster processing of big data.\n",
    "\n",
    "Write a paragraph comparing some of these technology options. For example, you could describe which technology you may personally prefer to use, in what circumstances, for what reasons.\n",
    "\n",
    "(You can add your paragraph as a Markdown cell at the bottom of your SageMaker Notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Answer\n",
    "\n",
    "Numba and dask are great tools to use if you already have experience using Pandas and Python. The syntax and logic are very similar to pandas and python. Numba and dask are useful when you want to parallelize the workflow. This will speed up your workflow when transforming and cleaning data. The order of speed is: Numba > Dask > Python. The speed difference can be seen when running similar processes with Numba or Dask and using the python magic function %time.\n",
    "\n",
    "Even though the AWS ecosystem costs can accumulate, I prefer AWS over other tools like Databricks. AWS tools can also have a steeper learning curve, but it will pay off in the long run because of the increased support and integration with the larger ecosystem of AWS tools. Databricks is good to use as a standalone product, has easy installation of notebooks and clusters and it is tough to beat the cost, free. But, it can be frustrating when you run into an error statement and the solution is not as obvious. As a beginning developer, and probably a developer at any stage, having a good linter can save you a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
