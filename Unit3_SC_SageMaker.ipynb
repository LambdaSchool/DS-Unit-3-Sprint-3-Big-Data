{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. SageMaker & Dask\n",
    "In this part, you'll work with a dataset of [YouTube Spam Comments](https://christophm.github.io/interpretable-ml-book/spam-data.html).\n",
    "\n",
    "> We work with 1956 comments from 5 different YouTube videos. The comments were collected via the YouTube API from five of the ten most viewed videos on YouTube in the first half of 2015. All 5 are music videos. One of them is “Gangnam Style” by Korean artist Psy. The other artists were Katy Perry, LMFAO, Eminem, and Shakira.\n",
    "\n",
    "> The comments were manually labeled as spam or legitimate. Spam was coded with a “1” and legitimate comments with a “0”.\n",
    "\n",
    "### Notebook\n",
    "Create a new notebook, with the **conda_python3** kernel.\n",
    "\n",
    "For this Sprint Challenge, you *don't* need to create a Dask Distributed Client. You can just use a Dask Dataframe.\n",
    "\n",
    "Load the five csv files into one Dask Dataframe. It should have a length of 1956 rows, and 5 columns.\n",
    "\n",
    "Use the Dask Dataframe to compute the counts of spam (1005 comments) versus the counts of legitimate comments (951).\n",
    "\n",
    "Spammers often tell people to check out their stuff! When the comments are converted to lowercase, then 461 spam comments contain the word \"check\", versus only 19 legitimate comments which contain the word \"check.\" Use the Dask Dataframe to compute these counts.\n",
    "\n",
    "### Optional bonus\n",
    "To score a 3, do extra work, such as creating the Dask Distributed Client, or creating a visualization with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:40617\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>16</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>67.53 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:40617' processes=16 cores=16>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=16)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS'], dtype='object')\n",
      "1956\n"
     ]
    }
   ],
   "source": [
    "# Read in the csv's with dask dataframe\n",
    "\n",
    "yt_comments = dd.read_csv('Youtube*.csv')\n",
    "print(yt_comments.columns)\n",
    "print(len(yt_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n"
     ]
    }
   ],
   "source": [
    "spam = yt_comments[yt_comments['CLASS'] == 1]\n",
    "\n",
    "print(len(spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951\n"
     ]
    }
   ],
   "source": [
    "legit = yt_comments[yt_comments['CLASS'] == 0]\n",
    "\n",
    "print(len(legit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_spam = spam[spam['CONTENT'].str.lower().str.contains('check')]\n",
    "\n",
    "len(check_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_legit = legit[legit['CONTENT'].str.lower().str.contains('check')]\n",
    "\n",
    "len(check_legit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Big data options\n",
    "You've been introduced to a variety of platforms (AWS SageMaker, AWS EMR, Databricks), libraries (Numba, Dask, MapReduce, Spark), and languages (Python, SQL, Scala, Java) that can \"scale up\" or \"scale out\" for faster processing of big data.\n",
    "\n",
    "Write a paragraph comparing some of these technology options. For example, you could describe which technology you may personally prefer to use, in what circumstances, for what reasons.\n",
    "\n",
    "(You can add your paragraph as a Markdown cell at the bottom of your SageMaker Notebook.)\n",
    "\n",
    "### Optional bonus\n",
    "To score a 3, create a diagram comparing some of these technology options, or a flowchart to illustrate your decision-making process. \n",
    "\n",
    "You can use text-based diagram tools, such as:\n",
    "- https://www.tablesgenerator.com/markdown_tables\n",
    "- https://mermaidjs.github.io/mermaid-live-editor/\n",
    "\n",
    "Or you can use presentation or drawing software, and commit your diagram to your GitHub repo as an image file. Or sketch on the back of a napkin, and take a photo with your phone. (If you choose to create a diagram, then you should also consider publishing it with a blog post later, after the Sprint Challenge.)\n",
    "\n",
    "### GitHub\n",
    "Commit your SageMaker notebook for parts 1 & 2 to GitHub. You can use git directly from the SageMaker terminal. Or you can download the .ipynb file from SageMaker to your local machine, and then commit the file to GitHub.\n",
    "\n",
    "### Stop your instance\n",
    "Stop your SageMaker Notebook instance, so you don't use excessive AWS credits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "From all the work we've been doing and from previous experience, I would prefer not to use databricks. While the notebooks and clusters are a little easier to use, the errors it gives are rarely very helpful and it fails to assist in writing the code (ie, not always giving the ending quote, curly-brace, parenthesis). While AWS SageMaker and EMR may be more expensive, there is definitely more support and development happening which causes them to be better tools in the big data space.\n",
    "\n",
    "When working with big data,I would definitely choose Dask for getting more processing power. You can distribute the work across the number of workers you choose to define. It's very familiar since it uses features in the same format as the python libraries we've learned, so it's more intuitive than Spark.\n",
    "\n",
    "As far as languages, I prefer to use SQL just because it is what is most familiar to me, but I have enjoyed using it with Python and Scala. I don't feel Scala's DF API is very intuitive and have resorted to using spark.sql. I definitely prefer working with Python over Scala. So the combination of Python + SQL would be my choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
